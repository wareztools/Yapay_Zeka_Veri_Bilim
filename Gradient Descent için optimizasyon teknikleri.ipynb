{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Gradient Descent için optimizasyon teknikleri.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNU93AQGWqZ/2veqOCyGqkv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Gradient Descent , bir fonksiyon için minimum değeri bulmak için kullanılan yinelemeli bir optimizasyon algoritmasıdır. Genel fikir, parametreleri rastgele değerlere başlatmak ve ardından her yinelemede \"eğim\" yönünde küçük adımlar atmaktır. Degrade iniş, hata işlevini en aza indirmek ve parametreler için en uygun değerleri bulmak için denetimli öğrenmede yüksek oranda kullanılır. Gradyan iniş algoritmaları için çeşitli uzantılar tasarlanmıştır. Bazıları aşağıda tartışılmaktadır:\n","\n","\n"],"metadata":{"id":"5jP1jPh6vxdI"}},{"cell_type":"markdown","source":["##Momentum Metod\n","\n","Momentum yöntemi : Bu yöntem, gradyanların üstel ağırlıklı ortalamasını dikkate alarak gradyan iniş algoritmasını hızlandırmak için kullanılır. Ortalamaları kullanmak, olağandışı yönlere doğru olan gradyanlar iptal edildiğinden, algoritmanın minimuma daha hızlı yakınsamasını sağlar. Momentum yönteminin sözde kodu aşağıda verilmiştir.\n","\n","```\n","V = 0\n","for each iteration i:\n","    compute dW\n","    V = β V + (1 - β) dW\n","    W = W - α V\n","\n","```"],"metadata":{"id":"m1Lovszykg-p"}},{"cell_type":"markdown","source":["**RMSprop :** RMSprop, Toronto Üniversitesi'nden Geoffrey Hinton tarafından önerildi. Sezgi, gradyanların ikinci momentine (dW 2 ) üstel ağırlıklı bir ortalama yöntemi uygulamaktır .\n","\n","```\n","S = 0\n","for each iteration i\n","    compute dW\n","    S = β S + (1 - β) dW2\n","    W = W - α dW⁄√S + ε\n","```\n","**Adam Optimizasyonu :** Adam optimizasyon algoritması, momentum yöntemini ve RMSprop'u, önyargı düzeltmesiyle birlikte içerir.\n","```\n","V = 0\n","S = 0\n","for each iteration i\n","    compute dW\n","    V = β1 S + (1 - β1) dW\n","    S = β2 S + (1 - β2) dW2\n","    V = V⁄{1 - β1i}\n","    S = S⁄{1 - β2i}\n","    W = W - α V⁄√S + ε\n","\n","\n","```\n","**Adam Optimizasyonu için önerilen parametre değerleri**\n","\n","α = 0,001 \n","\n","β 1 = 0,9 \n","\n","β 2 = 0,999 \n","\n","ε = 10 -8"],"metadata":{"id":"GzdEWBqdl-IN"}},{"cell_type":"markdown","source":["V ve dW sırasıyla hız ve ivmeye benzer. α öğrenme oranıdır ve β normalde 0,9'da tutulan momentuma benzer. Fizik yorumu, yokuş aşağı yuvarlanan bir topun hızının, tepenin eğim (gradyan) yönüne göre momentum oluşturduğu ve bu nedenle topun minimum bir değerde (bizim durumumuzda - minimum kayıpta) daha iyi ulaşmasına yardımcı olduğu şeklindedir.\n"],"metadata":{"id":"_N_chuWxv3hN"}}]}