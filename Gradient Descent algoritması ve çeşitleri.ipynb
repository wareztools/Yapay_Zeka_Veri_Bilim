{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Gradient Descent algoritması ve çeşitleri.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPniVZow27QE78um1cvN2WW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Gradient Descent, çeşitli makine öğrenme algoritmalarında maliyet fonksiyonunu en aza indirmek için kullanılan bir optimizasyon algoritmasıdır. Temel olarak öğrenme modelinin parametrelerini güncellemek için kullanılır."],"metadata":{"id":"voG1sWPyTQQN"}},{"cell_type":"markdown","source":["**Gradyan İniş Türleri:**\n","\n","1. Toplu Gradyan İnişi: \n","\n","Bu, her bir gradyan inişi yinelemesi için tüm eğitim örneklerini işleyen bir gradyan inişi türüdür. \n","\n","Ancak eğitim örneklerinin sayısı fazlaysa, toplu gradyan inişi hesaplama açısından çok pahalıdır. \n","\n","Bu nedenle, eğitim örneklerinin sayısı büyükse, toplu gradyan inişi tercih edilmez. \n","\n","Bunun yerine, stokastik gradyan inişi veya mini-parti gradyan inişi kullanmayı tercih ediyoruz.\n","2. **Stokastik Gradyan İnişi:**\n","\n","Bu, yineleme başına 1 eğitim örneğini işleyen bir tür gradyan inişidir.\n","\n"," Bu nedenle, yalnızca tek bir örneğin işlendiği bir yinelemeden sonra bile parametreler güncellenmektedir. \n"," \n"," Dolayısıyla bu, toplu gradyan inişinden oldukça hızlıdır. \n"," \n"," Ancak yine, eğitim örneklerinin sayısı çok olduğunda, yineleme sayısı oldukça fazla olacağından, sistem için ek yük oluşturabilecek yalnızca bir örneği işler.\n","\n","3. **Mini Toplu gradyan inişi:**\n","\n"," Bu, hem toplu gradyan inişinden hem de stokastik gradyan inişinden daha hızlı çalışan bir gradyan inişi türüdür. Burada b<m'nin yineleme başına işlendiği b örnekleri . Yani eğitim örneklerinin sayısı çok olsa bile, tek seferde b eğitim örnekleri yığınları halinde işlenir. Bu nedenle, daha büyük eğitim örnekleri için ve bu da daha az sayıda yineleme ile çalışır.\n","\n","**Kullanılan Değişkenler:**\n","\n","Eğitim örneklerinin sayısı m olsun.\n","\n","Özellik sayısı n olsun.\n","\n","**Not:** b == m ise, mini toplu gradyan inişi, toplu gradyan inişine benzer şekilde davranacaktır.\n","\n","\n"],"metadata":{"id":"wgxCTk3iEbGY"}},{"cell_type":"markdown","source":["**Toplu (Kesikli) gradyan inişi için algoritma:**\n","\n","h θ (x) lineer regresyon için hipotez olsun. Daha sonra maliyet fonksiyonu şu şekilde verilir:\n","\n","Σ, i=1'den m'ye kadar olan tüm eğitim örneklerinin toplamını temsil etsin.\n","\n","\n","```\n","\n","Jtrain(θ) = (1/2m) Σ( hθ(x(i))  - y(i))2\n","\n","Repeat {\n"," θj = θj – (learning rate/m) * Σ( hθ(x(i))  - y(i))xj(i)\n","    For every j =0 …n \n","}\n","\n","```\n","\n","Burada x j (i) i . eğitim örneğinin j . özelliğini temsil eder. Dolayısıyla, m çok büyükse (örneğin 5 milyon eğitim örneği), global minimuma yakınsamak saatler hatta günler alır. Bu nedenle, büyük veri kümeleri için, öğrenmeyi yavaşlattığı için toplu gradyan inişi kullanılması tavsiye edilmez.\n","\n","**Stokastik gradyan inişi için algoritma:**\n","\n","1) Parametrelerin her veri türü için eşit olarak eğitilebilmesi için veri setini rastgele karıştırın.\n","\n","2) Yukarıda belirtildiği gibi, yineleme başına bir örneği dikkate alır.\n","\n","Buradan,Eğitim örneği \n","```\n","Hence,\n","Let (x(i),y(i)) be the training example\n","Cost(θ, (x(i),y(i))) = (1/2) Σ( hθ(x(i))  - y(i))2\n","\n","Jtrain(θ) = (1/m) Σ Cost(θ, (x(i),y(i)))\n","\n","Repeat {\n","\n","For i=1 to m{\n","\n","         θj = θj – (learning rate) * Σ( hθ(x(i))  - y(i))xj(i)\n","        For every j =0 …n\n","\n","                } \n","}\n","```\n","\n","**Mini parti gradyan inişi için algoritma:**\n","\n","b, b < m olan bir partideki örneklerin sayısı olsun.\n","\n","b = 10, m = 100 varsayalım;\n","\n","Not: Ancak parti boyutunu ayarlayabiliriz. Genellikle 2'nin gücü olarak tutulur. Bunun nedeni, GPU'lar gibi bazı donanımların, 2'nin gücü gibi ortak parti boyutlarıyla daha iyi çalışma süresi elde etmesidir.\n","```\n","Repeat {\n"," For i=1,11, 21,…..,91\n","\n","    Let Σ be the summation from i to i+9 represented by k. \n","\n","    θj = θj – (learning rate/size of (b) ) * Σ( hθ(x(k))  - y(k))xj(k)\n","        For every j =0 …n\n","\n","}\n","```"],"metadata":{"id":"EqAUfxpIG_fk"}},{"cell_type":"markdown","source":["**Gradyan İnişlerinin farklı varyantlarında yakınsama eğilimleri:**\n","\n","Batch Gradient Descent durumunda, algoritma minimuma doğru düz bir yol izler. Maliyet fonksiyonu dışbükey ise global minimuma yakınsar ve maliyet fonksiyonu dışbükey değilse yerel minimuma yakınsar.\n","\n"," Burada öğrenme oranı tipik olarak sabit tutulur.\n","\n","Stokastik gradyan inişi ve mini-parti gradyan inişi durumunda, algoritma yakınsama yapmaz, ancak global minimum etrafında dalgalanmaya devam eder. Bu nedenle yakınsaması için öğrenme oranını yavaş yavaş değiştirmeliyiz. Ancak Stokastik gradyan inişinin yakınsaması, bir yinelemede olduğu gibi çok daha gürültülüdür, yalnızca bir eğitim örneğini işler."],"metadata":{"id":"ldFsNfmGIYGr"}}]}